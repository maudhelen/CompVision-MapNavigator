{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Import the necessary modules.\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom colors in BGR format\n",
    "dot_color = (255, 105, 180)  # Hot pink\n",
    "line_color = (0, 165, 255)   # Orange\n",
    "\n",
    "# Create DrawingSpec for landmarks and connections\n",
    "landmark_drawing_spec = mp.solutions.drawing_utils.DrawingSpec(color=dot_color, thickness=10, circle_radius=10)\n",
    "connection_drawing_spec = mp.solutions.drawing_utils.DrawingSpec(color=line_color, thickness=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1710333986.726340       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 76.3), renderer: Apple M1 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hand landmarks of images/closed.jpg:\n",
      "Annotated image saved as images/annotated_closed.jpg\n",
      "Hand landmarks of images/open.jpg:\n",
      "Annotated image saved as images/annotated_open.jpg\n",
      "Hand landmarks of images/right.jpg:\n",
      "Annotated image saved as images/annotated_right.jpg\n",
      "Hand landmarks of images/left.jpg:\n",
      "Annotated image saved as images/annotated_left.jpg\n",
      "Failed to load image images/left2\n",
      "Hand landmarks of images/right2.jpg:\n",
      "Annotated image saved as images/annotated_right2.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@3.971] global loadsave.cpp:248 findDecoder imread_('images/left2'): can't open/read file: check file path/integrity\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# STEP 1: Define your image paths (assuming they're in a folder named 'images')\n",
    "images = ['images/closed.jpg', 'images/open.jpg', 'images/right.jpg', 'images/left.jpg', 'images/left2.jpg', 'images/right2.jpg']\n",
    "\n",
    "# Create a MediaPipe Hands object.\n",
    "mp_hands = mp.solutions.hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "for image_path in images:\n",
    "    # Load the image.\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Failed to load image {image_path}\")\n",
    "        continue\n",
    "\n",
    "    # Convert the BGR image to RGB.\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the image.\n",
    "    results = mp_hands.process(image_rgb)\n",
    "\n",
    "    # Print the results.\n",
    "    print(f'Hand landmarks of {image_path}:')\n",
    "    \n",
    "    # Draw the hand landmarks on the image.\n",
    "    annotated_image = image.copy()\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Draw connections\n",
    "            mp_drawing.draw_landmarks(\n",
    "                annotated_image,\n",
    "                hand_landmarks,\n",
    "                mp.solutions.hands.HAND_CONNECTIONS,\n",
    "                landmark_drawing_spec,\n",
    "                connection_drawing_spec)\n",
    "            # Draw landmark points and numbers\n",
    "            for idx, landmark in enumerate(hand_landmarks.landmark):\n",
    "                landmark_px = mp_drawing._normalized_to_pixel_coordinates(landmark.x, landmark.y, image.shape[1], image.shape[0])\n",
    "                if landmark_px:  # Check if conversion is successful (it could be None)\n",
    "                    cv2.circle(annotated_image, landmark_px, 5, (255, 0, 0), -1)\n",
    "                    cv2.putText(annotated_image, str(idx), landmark_px, cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 3)\n",
    "\n",
    "    # Display and save the annotated image\n",
    "    # cv2.imshow(f'Annotated {image_path}', annotated_image)\n",
    "    annotated_image_path = f'images/annotated_{image_path.split(\"/\")[-1]}'\n",
    "    cv2.imwrite(annotated_image_path, annotated_image)\n",
    "    print(f'Annotated image saved as {annotated_image_path}')\n",
    "\n",
    "    # Use cv2.waitKey() if you're displaying images with cv2.imshow()\n",
    "    #cv2.waitKey(0)  # Wait for a key press to proceed, comment this line if running in a script without displaying\n",
    "\n",
    "# Close the MediaPipe Hands object\n",
    "mp_hands.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
